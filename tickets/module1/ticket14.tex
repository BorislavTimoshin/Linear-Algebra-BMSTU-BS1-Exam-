\section{
    Метод наименьших квадратов, обоснование, подробное описание, пример решения.
}

\subsection{
    Метод наименьших квадратов, обоснование, подробное описание.
}

Рассмотрим систему из $n$ линейных алгебраических уравнений (СЛАУ) относительно $k$ неизвестных

\begin{equation}
    \begin{cases}
        \begin{aligned}
            a_{11} x_1 + a_{21} x_2 + \ldots + a_{k1} x_k &= b_1 \\
            a_{12} x_1 + a_{22} x_2 + \ldots + a_{k2} x_k &= b_2 \\
            \hdotsfor{2} \\
            a_{1n} x_1 + a_{2n} x_2 + \ldots + a_{kn} x_k &= b_n \\
        \end{aligned}
    \end{cases}
    \label{eq:slau_14_1}
\end{equation}

Каждому набору неизвестных неизвестных сопоставим числа

$$d_i = b_i - (a_{1i}x_1 + \ldots + a_{ki}x_k), \quad i = \overline{1, n},$$

которые называют \textbf{невязками уравнений} системы.

Отметим, что функция 

$$f(x_1, \ldots, x_k) = \sum_{i=1}^{n} [b_i - (a_{1i}x_1 + \ldots + a_{ki}x_k)]^2$$

на решениях системы равна нулю и положительна в остальных случаях. Поэтому ее можно рассматривать как оценку отклонения набора значений неизвестных от точного решения системы. Если система несовместна, то часто возникает задача найти вместо отсутствующих решений такой набор значений неизвестных, который приводит к наименьшему значению функции $f$. Такой подход в решении задачи, не имеющей решения, называют \textbf{методом наименьших квадратов}, поскольку ищется минимум функции, являющейся суммой квадратов.

Будем трактовать столбцы коэффициентов при неизвестных, столбец свободных членов как столбцы координат векторов $\vec{a_1}, \ldots, \vec{a_k}, \vec{b}$ евклидова арифметического пространства $\RR^n$ в стандартном базисе. Тогда и набор невязок уравнений системы можно рассматривать в виде вектора $\vec{d} = \begin{pmatrix} d_1 \\ \vdots \\ d_n \end{pmatrix} \in \RR^n$.

$$\vec{d} = \vec{b} - (x_1\vec{a_1} + \ldots + x_k\vec{a_k}).$$

Число $\lVert \vec{d} \rVert$ \textbf{назовем невязкой СЛАУ} $\eqref{eq:slau_14_1}$. Вычислив скалярный квадрат вектора $\vec{d}$, находим

$$\lVert \vec{d} \rVert^2 = f(x_1, \ldots, x_k).$$

Значит, задача сводится к нахождению таких действительных коэффициентов $x_1, \ldots, x_k$, при которых $\lVert \vec{d} \rVert \to \min$.

Введем линейное подпространство $\mathcal{H} = \Span(\vec{a_1}, \ldots, \vec{a_k})$ и его ортогональное дополнение $\mathcal{H}^\perp$. Разложим вектор $\vec{b}$ на его ортогональную проекцию на линейное подпространство $\mathcal{H}$ и соответствующую ортогональную составляющую:

$$b = \vec{h} + \vec{h}^\perp, \quad \vec{h} \in \mathcal{H}, \thinspace \vec{h}^\perp \in \mathcal{H}^\perp.$$

Тогда

$$\vec{d} = \vec{h} + \vec{h}^\perp - (x_1\vec{a_1} + \ldots + x_k\vec{a_k}) = \vec{h}^\perp + (\vec{h} - x_1\vec{a_1} - \ldots - x_k\vec{a_k}) = \vec{h}^\perp + \vec{d_0},$$

где

$$\vec{d_0} = \vec{h} - x_1\vec{a_1} - \ldots - x_k\vec{a_k} \in \mathcal{H}.$$

Так как $\vec{d_0} \perp \vec{h}^\perp$, то по теореме Пифагора заключаем, что

$$\lVert \vec{d} \rVert^2 = \lVert \vec{d_0} \rVert^2 + \lVert \vec{h}^\perp \rVert^2.$$

Ортогональная составляющая $\vec{h}^\perp$ вектора невязок постоянна и от выбора коэффициентов $x_i$ не зависит. Поэтому минимизация величины $\lVert \vec{d} \rVert^2$ сводится к поиску минимума величины $\lVert \vec{d_0} \rVert^2$. Эта величина является неотрицательной и достигает минимума, если обращается в нуль, т.е. при условии, что $\vec{d_0} = \vec{0}$. А это равносильно тому, что $\vec{d} = \vec{h}^\perp$, т.е. вектор невязок принадлежит ортогональному дополнению $\mathcal{H}^\perp$ и поэтому является решением системы

\begin{equation}
    (\vec{a_j}, \vec{d}) = 0, \quad j = \overline{1, k},
    \label{eq:slau_14_2}
\end{equation}

или

$$(\vec{a_j}, \vec{b} - (x_1\vec{a_1} + \ldots + x_k\vec{a_k})), \quad j = \overline{1, k},$$

\bigbreak

После преобразований получаем СЛАУ

\begin{equation}
    \begin{cases}
        \begin{aligned}
            (\vec{a_1}, \vec{a_1}) x_1 + \ldots + (\vec{a_1}, \vec{a_k})x_k &= (\vec{a_1}, \vec{b}) \\
            \hdotsfor{2} \\
            (\vec{a_k}, \vec{a_1}) x_1 + \ldots + (\vec{a_k}, \vec{a_k})x_k &= (\vec{a_k}, \vec{b}) \\
        \end{aligned}
    \end{cases}
    \label{eq:slau_14_3}
\end{equation}

относительно неизвестных $x_1, \ldots, x_k$. Матрица этой системы $\Gamma = ((\vec{a_i}, \vec{a_j}))$ - это квадратная матрица порядка $k$, представляющая собой матрицу Грама для системы векторов $\vec{a_1}, \ldots, \vec{a_k}$. (\textbf{*}При этом данную СЛАУ вида $A^TA\vec{x} = A^T\vec{b}$ называют \textbf{нормальной}.)

СЛАУ $\eqref{eq:slau_14_3}$ всегда совместна: ее решениями являются коэффициенты разложения вектора $\vec{h} \in \mathcal{H} = \Span(\vec{a_1}, \ldots, \vec{a_k})$ по системе векторов $\vec{a_1}, \ldots, \vec{a_k}$, так как в этом случае вектор $\vec{d} = \vec{b} - \vec{h} = \vec{h}^\perp$ - решение системы $\eqref{eq:slau_14_2}$. Если система векторов $\vec{a_1}, \ldots, \vec{a_k}$ линейно независима, то матрица СЛАУ $\eqref{eq:slau_14_3}$ невырождена и эта система имеет единственное решение $x_1, \ldots, x_k$, которое дает решение исходной задачи. Если же указанная система векторов линейно зависима, то матрица СЛАУ $\eqref{eq:slau_14_3}$ вырождена. В этом случае квадратная СЛАУ $\eqref{eq:slau_14_3}$, будучи совместной, имеет бесконечно много решений $x_1, \ldots, x_k$ (\textbf{*}псевдорешений исходной СЛАУ $A\vec{x} = \vec{b}$) и каждое из них дает решение исходной задачи (т.е. $\lVert \vec{d} \rVert \to \min$). (\textbf{*}Среди этих решений можно выбирать те, которые удовлетворяют каким-то дополнительным условиям: минимальное по норме (нормальное псевдорешение) и др.).

\bigbreak

Пусть $\vec{x} = \alpha_1\vec{a}_1 + \ldots + \alpha_n\vec{a}_n$ - и есть то наилучшее приближение вектора $\vec{b}$, которое мы найдем.

\textbf{\textit{Абсолютная ошибка приближения}} $= \norm{\vec{b} - \vec{x}}$.

\textbf{\textit{Относительная ошибка приближения}} $= \frac{\norm{\vec{b} - \vec{x}}}{\norm{\vec{b}}}$.


\subsection{
    Пример решения.
}

$\vec{a}_1 = \begin{pmatrix} 1 \\ -2 \\ 2 \end{pmatrix}$, $\vec{a}_2 = \begin{pmatrix} 3 \\ 1 \\ -3 \end{pmatrix}$, $\vec{b} = \begin{pmatrix} 16 \\ 6 \\ 4 \end{pmatrix}$

$A = \begin{pmatrix} \vec{a}_1 & \vec{a}_2 \end{pmatrix} = \begin{pmatrix} 1 & 3 \\ -2 & 1 \\ 2 & -3 \end{pmatrix}.$

Найдем элементы матрицы Грама $\Gamma(\vec{a}_1, \vec{a}_2) = A^TA$:

\begin{gather*}
    (\vec{a}_1, \vec{a}_1) = 1 + 4 + 4 = 9 \\
    (\vec{a}_1, \vec{a}_2) = 3 - 2 - 6 = -5 \\
    (\vec{a}_2, \vec{a}_2) = 9 + 1 + 9 = 19
\end{gather*}


Найдем элементы матрицы $A\vec{b}$:

\begin{gather*}
    (\vec{a}_1, \vec{b}) = 16 - 12 + 8 = 12 \\
    (\vec{a}_2, \vec{b}) = 48 + 6 - 12 = 42
\end{gather*}

Найдем решение СЛАУ $A^TA\vec{x} = A\vec{b}$:

\begin{equation*}
    \left(\begin{array}{cc|c}
        9 & -5 & 12 \\
        -5 & 19 & 42
    \end{array}\right)
    \sim
    \left(\begin{array}{cc|c}
        -1 & 33 & 96 \\
        0 & -146 & -438
    \end{array}\right)
    \sim
    \left(\begin{array}{cc|c}
        1 & -33 & -96 \\
        0 & 1 & 3
    \end{array}\right)
    \sim
    \left(\begin{array}{cc|c}
        1 & 0 & 3 \\
        0 & 1 & 3
    \end{array}\right)
.\end{equation*}

$\vec{x} = 3\vec{a}_1 + 3\vec{a}_2 = \begin{pmatrix} 12 \\ -3 \\ -3 \end{pmatrix}$ – наилучшее приближение вектора $\vec{b}$.

Абсолютная ошибка $= \norm{\vec{b} - \vec{x}} = \norm{\begin{pmatrix} 16 \\ 6 \\ 4 \end{pmatrix} - \begin{pmatrix} 12 \\ -3 \\ -3 \end{pmatrix}} =  \norm{\begin{pmatrix} 4 \\ 9 \\ 7 \end{pmatrix}} =  \sqrt{146}$.

Относительная ошибка $= \frac{\norm{\vec{b} - \vec{x}}}{\norm{\vec{b}}} = \frac{\sqrt{146}}{\sqrt{308}} = \frac{\sqrt{11242}}{154}$.
